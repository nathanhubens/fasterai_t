{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the network sparse ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision import *\n",
    "from fastai.core import *\n",
    "from fastai.callbacks import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](imgs/pruning.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsifier():\n",
    "    def __init__(self, model, granularity, method, criteria):\n",
    "        self.granularity, self.method, self.criteria, self.model = granularity, method, criteria, model\n",
    "        self._save_weights() # Save the original weights\n",
    "        \n",
    "    def prune(self, sparsity):\n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if self.criteria == 'l1':\n",
    "                    weight = self._l1_norm(m.weight)\n",
    "                elif self.criteria == 'grad':\n",
    "                    weight = self._grad_crit(m.weight)\n",
    "                elif self.criteria == 'movement':\n",
    "                    weight = self._movement(m)\n",
    "                else: raise NameError('Invalid Criteria')\n",
    "                \n",
    "                mask = self._compute_mask(self.model, weight, sparsity)\n",
    "                mask = make_broadcastable(mask, m.weight) # Reshape the mask to be broadcastable with the weights\n",
    "                m.register_buffer(\"_mask\", mask) # Put the mask into a buffer\n",
    "                self._apply(m) \n",
    "    \n",
    "    def _apply(self, module):\n",
    "        '''\n",
    "        Apply the mask and freeze the gradient so the corresponding weights are not updated anymore\n",
    "        '''\n",
    "        mask = getattr(module, \"_mask\")\n",
    "        module.weight.data.mul_(mask)\n",
    "        if module.weight.grad is not None: # In case some layers are freezed\n",
    "            module.weight.grad.mul_(mask)\n",
    "    \n",
    "    def _l1_norm(self, weight):\n",
    "\n",
    "        if self.granularity == 'weight':\n",
    "            w = weight.view(-1).abs().clone()\n",
    "            \n",
    "        elif self.granularity == 'vector':\n",
    "            w = torch.norm(weight, p=1, dim=(3)).view(-1)/(weight.shape[3]) # Normalize the norm to be consistent for different dimensions\n",
    "\n",
    "        elif self.granularity == 'kernel':\n",
    "            w = torch.norm(weight, p=1, dim=(2,3)).view(-1)/(weight.shape[2]*weight.shape[3]) \n",
    "        \n",
    "        elif self.granularity == 'filter':       \n",
    "            w = torch.norm(weight, p=1, dim=(1,2,3))/(weight.shape[1]*weight.shape[2]*weight.shape[3])\n",
    "\n",
    "        else: raise NameError('Invalid Granularity') \n",
    "        \n",
    "        return w\n",
    "        \n",
    "    def _grad_crit(self, weight):\n",
    "        if weight.grad is not None:\n",
    "            if self.granularity == 'weight':\n",
    "                w = (weight*weight.grad).data.pow(2).view(-1)\n",
    "\n",
    "            elif self.granularity == 'vector':\n",
    "                w = (weight*weight.grad).data.pow(2).sum(dim=(3)).view(-1).clone()/(weight.shape[3])\n",
    "\n",
    "            elif self.granularity == 'kernel':\n",
    "                w = (weight*weight.grad).data.pow(2).sum(dim=(2,3)).view(-1).clone()/(weight.shape[2]*weight.shape[3])    \n",
    "                \n",
    "            elif self.granularity == 'filter':       \n",
    "                w = (weight*weight.grad).data.pow(2).sum(dim=(1,2,3))/(weight.shape[1]*weight.shape[2]*weight.shape[3])\n",
    "\n",
    "            else: raise NameError('Invalid Granularity') \n",
    "\n",
    "            return w\n",
    "        \n",
    "    def _movement(self, module):\n",
    "        if hasattr(module, '_old_weights') == False:\n",
    "            module.register_buffer(\"_old_weights\", module._init_weights.clone()) # If the previous value of weights is not known, take the initial value\n",
    "            \n",
    "        old_weights = getattr(module, \"_init_weights\")\n",
    "\n",
    "        if self.granularity == 'weight': \n",
    "            w = torch.abs((module.weight.view(-1)).clone()) - torch.abs(old_weights.view(-1).clone())\n",
    "\n",
    "        elif self.granularity == 'vector': \n",
    "            w = torch.abs(module.weight.sum(dim=(3)).clone()) - torch.abs(old_weights.sum(dim=(3).clone()))\n",
    "\n",
    "        elif self.granularity == 'kernel': \n",
    "            w = torch.abs(module.weight.sum(dim=(2,3)).clone()) - torch.abs(old_weights.sum(dim=(2,3).clone()))           \n",
    "        \n",
    "        elif self.granularity == 'filter': \n",
    "            w = torch.abs(module.weight.sum(dim=(1,2,3)).clone()) - torch.abs(old_weights.sum(dim=(1,2,3).clone()))\n",
    "\n",
    "        else: raise NameError('Invalid Granularity')\n",
    "\n",
    "        module._old_weights = module.weight.clone() # The current value becomes the old one for the next iteration\n",
    "            \n",
    "        return w\n",
    "    \n",
    "    def _reset_weights(self):\n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init_weights = getattr(m, \"_init_weights\")\n",
    "                m.weight.data = init_weights.clone()\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init_weights = getattr(m, \"_init_weights\")\n",
    "                m.weight.data = init_weights.clone()\n",
    "                self._apply(m) # Reset the weights and apply the current mask\n",
    "                \n",
    "    def _save_weights(self):\n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                m.register_buffer(\"_init_weights\", m.weight.clone())\n",
    "                    \n",
    "    \n",
    "    def _compute_mask(self, model, weight, sparsity):\n",
    "        '''\n",
    "        Compute the binary masks\n",
    "        '''\n",
    "        if self.method == 'global':\n",
    "            global_weight = []\n",
    "            \n",
    "            for k, m in enumerate(model.modules()):\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    if self.criteria == 'l1':\n",
    "                        w = self._l1_norm(m.weight)\n",
    "                    elif self.criteria == 'grad':\n",
    "                        w = self._grad_crit(m.weight)\n",
    "                        \n",
    "                    global_weight.append(w)\n",
    "\n",
    "            global_weight = torch.cat(global_weight)\n",
    "            threshold = torch.quantile(global_weight, sparsity/100) # Compute the threshold globally\n",
    "            \n",
    "        elif self.method == 'local': \n",
    "            threshold = torch.quantile(weight, sparsity/100) # Compute the threshold locally\n",
    "            \n",
    "        else: raise NameError('Invalid Method')\n",
    "            \n",
    "        # Make sure we don't remove every weight of a given layer\n",
    "        if threshold > weight.max(): threshold = weight.max()\n",
    "\n",
    "        mask = weight.ge(threshold).to(dtype=weight.dtype)\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_broadcastable(input, target):\n",
    "    target_shape = target.shape\n",
    "    output_shape = [*target.shape]\n",
    "    \n",
    "    for i in range(len(target_shape)):\n",
    "        input_size = np.prod(input.shape)\n",
    "        target_size = np.prod(np.array(target_shape[:i+1]))\n",
    "        if input_size >= target_size:\n",
    "            output_shape[i]=target_shape[i]\n",
    "        else:\n",
    "            output_shape[i]=1\n",
    "        \n",
    "    new_input = input.reshape(*output_shape)        \n",
    "    return new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsifyCallback(LearnerCallback):\n",
    "    '''\n",
    "    sparsity: The percentage of sparsity you want in your final model (between 0 and 100)\n",
    "    granularity: The granularity the pruning will be operating on ('weights', 'vector', 'kernel', 'filters')\n",
    "    method: The method of selection of the parameters ('local' or 'global')\n",
    "    criteria: The criteria of selection ('l1', 'grad', 'movement')\n",
    "    sched_func: The scheduling function for the pruning ('one_shot', 'iterative', 'annealing_cos', 'gradual', ...)\n",
    "    start_epoch: The epoch you want to start pruning the network\n",
    "    start_reset: When doing Lottery Ticket Hypothesis, the epoch you want to start resetting weights to their original values (set to 0 if you don't want to reset the weights)\n",
    "    rewind: When doing Lottery Ticket Hypothesis with Rewind, the epoch you want to reset you weights to.\n",
    "    reset_end: If you want to reset your weights at the end of training to get your winning ticket.\n",
    "    '''\n",
    "        \n",
    "    def __init__(self, learn:Learner, sparsity, granularity, method, criteria, sched_func, start_epoch=0, lth_reset=False, rewind_epoch=0, reset_end=False):\n",
    "        super().__init__(learn)\n",
    "        self.sparsity, self.granularity, self.method, self.criteria, self.sched_func = sparsity, granularity, method, criteria, sched_func\n",
    "        self.reset_end, self.rewind_epoch, self.start_epoch, self.lth_reset = reset_end, rewind_epoch, start_epoch, lth_reset\n",
    "        self.sparsifier = Sparsifier(self.learn.model, self.granularity, self.method, self.criteria)\n",
    "        self.batches = math.floor(len(learn.data.train_ds)/learn.data.train_dl.batch_size)\n",
    "        self.current_sparsity, self.previous_sparsity = 0,0\n",
    "\n",
    "        assert self.start_epoch>self.rewind_epoch, 'You must rewind to an epoch before the start of the pruning process'\n",
    "    \n",
    "    def on_train_begin(self, n_epochs:int, **kwargs):\n",
    "        print(f'Pruning of {self.granularity} until a sparsity of {self.sparsity}%')\n",
    "        self.total_iters = n_epochs * self.batches\n",
    "        self.start_iter = self.start_epoch * self.batches\n",
    "        \n",
    "    def on_epoch_end(self, epoch, **kwargs):\n",
    "        print(f'Sparsity at the end of epoch {epoch}: {self.current_sparsity:.2f}%')\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, **kwargs):\n",
    "        if epoch == self.rewind_epoch:\n",
    "            print(f'Saving Weights at epoch {epoch}')\n",
    "            self.sparsifier._save_weights()\n",
    "        \n",
    "    def on_batch_begin(self, iteration, epoch, **kwargs):\n",
    "        if epoch>=self.start_epoch:\n",
    "            self.set_sparsity(iteration)\n",
    "            self.sparsifier.prune(self.current_sparsity)\n",
    "\n",
    "            if self.lth_reset and self.current_sparsity!=self.previous_sparsity: # If sparsity has changed, the network has been pruned\n",
    "                    print(f'Resetting Weights to their epoch {self.rewind_epoch} values')\n",
    "                    self.sparsifier._reset_weights()\n",
    "\n",
    "        self.previous_sparsity = self.current_sparsity\n",
    "        \n",
    "    def set_sparsity(self, iteration):\n",
    "        self.current_sparsity = self.sched_func(start=0., end=self.sparsity, pct=(iteration-self.start_iter)/(self.total_iters-self.start_iter))\n",
    "    \n",
    "    def on_train_end(self, **kwargs):\n",
    "        print(f'Final Sparsity: {self.current_sparsity:.2f}')\n",
    "        if self.reset_end:\n",
    "            self.sparsifier._reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
